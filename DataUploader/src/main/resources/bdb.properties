#동시에 실행될 Thread 수
NumOfThread=5

# storagetype
# 0 : local file system
# 1 : hdfs file system
StorageType=1

#Location= 읽어올 데이터가 있는 위치
#hdfs 나 local이나 schema 정보를 빼고 적어준다.
#스키마란 file:///또는 hdfs://
#Location=/product/ssa/user_info
Location=/syrup_info

HdfsConf=/home/sofangel/dev/hadoop/etc/hadoop
#HdfsConf=/app/home/bdb/hdfs_conf
# hdfs-site.xml 과 core-site.xml이 필요.

#hadoop cluster의 url
#HdfsUrl=DICc-m001:8020
HdfsUrl=localhost:8020

#Driver 정보
#모든 Driver를 지원함 대신 build가 필요하다.
Driver=org.apache.phoenix.jdbc.PhoenixDriver
#Driver=com.skplanet.querycache.jdbc.QCDriver


#phoenix 접속 String주소.
#ConString=jdbc:phoenix:BDBc-m003:2181
#ConString=jdbc:daas-phoenix://bdb04:8655
ConString=jdbc:phoenix:localhost:2181

#실행될 Query 구문. Upsert 또는 Insert 또는 update가 될수 있음.
#Query= upsert into ssa.syrupinfo values (?,?,?,?,?)

Query1=upsert into
Query2=values (?,?,?,?,?)
#  istore.userinfo

Table= istore.syrupinfo

#Type phoenix.apache.org/languase/datatypes.html
# INTEGER,
# UNSIGNED_INT
# BIGINT
# UNSIGNED_LONG
# TINYINT
# UNSIGNED_TINYINT
# SMALLINT
# UNSIGNED_SMALLINT
# FLOAT
# UNSIGNED_FLOAT
# DOUBLE
# UNSIGNED_DOUBLE치
# DECIMAL
# BOOLEAN
# TIME
# DATE
# TIMESTAMP
# UNSIGNED_TIME
# UNSIGNED_DATE
# UNSIGNED_TIMESTAMP
# VARCHAR
# CHAR
# BINARY
# VARBINARY
Type=VARCHAR,VARCHAR,VARCHAR,VARCHAR,VARCHAR

#
Delimiter=\t

#Hbase Compaction 정보
Compaction=true
